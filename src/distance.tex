%!TEX root = ../main.tex

\chapter{Distance estimation\label{chap:p3p}}

For distance estimation, one can leverage many different approaches, either directly relying on the event data stream itself, or integrating the
events over time (thus producing a grayscale image or a heatmap), and then estimating the position from the produced image. As we have shown in \refchap{chap:response}, the number of events
generated by LED sources decreases monotonically with the square of the distance and also decreases with increasing modulation frequency.
We could then assume that a simple distance predictor could be made by simply fitting a curve to the training dataset consisting of an average
number of events per blinking period (thus training this predictor for average number of events w. r. t. to distance) and then making the prediction of distance by calculating the average in real time.

This is possible to do in very specific cases where the camera settings and the scene lighting conditions do not change between training measurements
and the deployment. For example, changing the \texttt{bias-on} or \texttt{bias-off} settings of the camera changes the brightness change threshold on
which the camera generates events.
This changes the number of events that are generated without any information on the distance from the camera, this is similar to changing the eposure
time on a global shutter camera, which can then give an under- or over-exposed frame.
It also does not generalize the problem of estimating the position of an arbitrary UAV marked with LED lights and a camera with previously unspecified
settings. For a more robust way of estimating the pose of the UAV from the camera, we can leverage the following methods.

\section{VLP}
TODO: WRITE THIS

\section{RSSR}
TODO: WRITE THIS

\section{Perspective-n-Point}
TODO: WRITE THIS
