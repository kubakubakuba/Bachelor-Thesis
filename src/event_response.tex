%!TEX root = ../main.tex

\chapter{Response of an event-based camera\label{chap:response}}

\section{Event-based cameras compared to frame-based cameras}

Traditional frame-based (sometimes called global shutter) cameras capture the scene as a sequence of still image frames at fixed intervals with fixed settings, providing a synchronous representation of the visual world. 
In terms of ease of use and the simplicity of the post-processing of data obtained from such cameras, they are wildly applicable across many fields.
A single frame obtained from a frame-based camera may be described by the following equation \refeq{eq:frame} \cite{scheerlinck2018event}
\begin{equation}
Y_j(\boldsymbol{p}) := \frac{1}{\epsilon} \int_{t_j - \epsilon}^{t_j} Y(\boldsymbol{p}, \tau) \mathrm{d} \tau, \quad j \in 1, 2, 3 ...
\label{eq:frame}
\end{equation}
where $Y(\boldsymbol{p}, t)$ denotes the irradiation intensity of a camera pixel at a specific time $t$, $t_j$ is the time-stamp of the image capture and $\epsilon$ is the exposure time.
As we can see, each frame is represented by a temporal average of irradiance over the exposure time $\epsilon$. Although this model simplifies image formation, it introduces artifacts such as motion blur, particularly when fast-moving objects are captured with an exposure time mismatched to their dynamics.

\ac{DVS} (or event-based cameras), are vision sensors that draw their inspiration from nature bio-receptors, where each pixel reacts
to the change of illumination in the scene. Each pixel individually recognizes the log intensity and compares it to the previously
recorded value. When a predefined threshold is crossed, this value is reset to the current one and a new event is generated. This event can be expressed as $e = \begin{bmatrix} x & y & \sigma & t \end{bmatrix}$, where $\begin{bmatrix} x & y \end{bmatrix}$
is the camera pixel coordinate, $\sigma$ is the polarity of change (where $\sigma = \pm 1$ for increasing or decreasing change, respectively) and $t$ is the timestamp of the event. \cite{gallego22event} \cite{scheerlinck2018event} We can model the single event as a Dirac-delta function $\delta(t)$ and can define an event stream
$e_i(\boldsymbol{p}, t)$ at a pixel $\boldsymbol{p}$ by \refeq{eq:event_eq} \cite{scheerlinck2018event}
\begin{equation}
e_i(\boldsymbol{p}, t) := \sigma_i^{\boldsymbol{p}} c \, \delta(t - t_i^{\boldsymbol{p}}), \ i \in 1, 2, 3 ...
\label{eq:event_eq}
\end{equation}
where $\sigma_i^{\boldsymbol{p}}$ is the polarity and $t_i^{\boldsymbol{p}}$ is the time-stamp of the $i$-th event at a pixel.
The magnitude $c$ is the contrast threshold, a preset constant (similar to exposure time in frame-based cameras), which defines a change in light intensity that is encoded by a singular event, at each pixel. Event-based cameras thus circumvent many common issues found in traditional frame-based cameras, such as the motion blur mentioned before. They offer significant advantages, including high dynamic range, low latency,
and energy efficiency.
This makes them perfect for the application of agile robotics,
where the fast response time is crucial (especially in UAV swarming situations). With their submillisecond response time,
event-based cameras can provide a significant advantage over traditional cameras in these applications.
However, they also come with some drawbacks, such as the need for a different approach to
data processing (images can be reconstructed from the event stream by simply integrating the events over time,
making the usage of normal vision algorithms possible, but it also goes against the main advantage of event-based cameras)
and the higher cost of the camera units themselves. \cite{gallego22event}

\section{Equipment used}

\subsection{Event-based camera}
The event-based camera used in this thesis is the \texttt{Prophesee EVK4 HD}\footnote{Prophesee EVK4 website: \url{https://www.prophesee.ai/event-camera-evk4/}.},
with a \texttt{IMX636} sensor. The camera features a resolution of $1280 \times 720$ pixels and is capable of generating $1.066\times10^9$ events per second (equivalent to $10.000$ \ac{FPS}) and offers dynamic range of $120$ dB.
During recording, many settings of the camera can be set, such as the \ac{ROI} settings and the bias settings.

\subsubsection{ROI}
The \ac{ROI} setting during recording takes a rectangular region in pixel coordinates and ignores any generated events from the camera outside of
these coordinates. This setting is especially useful in cases where only a small static area needs to be observed or where many unrelated events
may be generated, that would interfere with the measurement process.

\subsubsection{Bias settings}
The bias settings are a global camera setting parameters such as an ISO value for analogue film or digital camera ISO sensitivity, or exposure time setting.
The configurable biases available for the \texttt{EVK4} can be explained as\cite{dilmaghani2022controlevaluationeventcameras}
\begin{itemize}
    \item \texttt{bias\_diff\_on} adjusts the threshold on which events are generated, with higher setting, the more increasing change in pixel brightness needs to be present to trigger a generation of event with positive $p$
    \item \texttt{bias\_diff\_off} adjusts the threshold on which events are generated, with higher setting, the more decreasing change in pixel brightness needs to be present to trigger a generation of event with negative $p$
    \item \texttt{bias\_fo} adjusts the low-pass filter, which filters out the fast fluctuations in light intensity, effectively setting the maximum detectable oscillation frequency
    \item \texttt{bias\_hpf} adjusts the high-pass filter, which filters out the slow fluctuations in light intensity, setting the minimum detectable oscillation frequency
    \item \texttt{bias\_refr} adjusts the pixel refractory period, in which a pixel is inactive after generating an event
\end{itemize}
These settings are leveraged while measuring data to filter out a large part of generated noise, as well as unneeded events, such as in brightly illuminated scenes, that produce a lot of event, with most of them not needed for the pose estimation.

\subsection{Lenses}
During the measurements, a fish eye lens with an integrated \ac{UV} filter was used to target the specific wavelength of the \ac{LED}s
that are used in the UVDAR localization system.

\subsection{UAVs}
The \ac{UAV} used for the measurements was an X500 unit from the \ac{MRS} UVDAR system, with \ac{UV} \ac{LED} light sources,
mounted on each arm of the \ac{UAV}, which can be modulated at specified frequencies and are used for localization and communication purposes.
Both can be seen in \reffig{fig:uavcam}.

\begin{figure}[H]
	\centering
	\subfloat[The event-based camera EVK4 from Prophesee with a 2.5mm fish eye lens.] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/evk4.jpg}
	  \label{fig:evk4}
	}
	\subfloat[X500 UAV unit equipped with UVDAR] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/uav1.jpeg}
	  \label{fig:uav1}
	}
	\caption{
  An event-based camera with a 2.5mm fish eye lens can be seen on \reffig{fig:evk4}, which was used to measure the UV LEDs mounted on the X500 UAV unit equipped with UVDAR as seen on \reffig{fig:uav1}.
  }
	\label{fig:uavcam}
\end{figure}

\section{Data collection}

The data used in this chapter were collected on several occasions by measuring a stationary \ac{UAV} at various distances and rotations from the camera.
Each \ac{UAV} is equipped with 8 \ac{UV} \ac{LED}s, with 2 \ac{LED}s on each arm of the \ac{UAV}. Each of the \ac{LED}s can be individually controlled
and can be set to various blinking sequences, with a common modulation frequency set for all \ac{LED}s.

\subsection{Initial measurements}

The initial measurements were made by securing the event-based camera on a tripod and placing the \ac{UAV} at distances ranging from
$0.5$ to $2.5$ meters. The \ac{LED}s were set to blink at a frequency in the range of $1$ Hz to $30$ kHz. No \ac{ROI} was set at this time
and the entire visible area was recorded during the testing.

This first experiment proved to be rather inefficient as the \ac{LED}s need to be isolated from each other's influence, which
was not done properly at this time. This problem is partially solvable in the post-processing, by filtering out the events
with \ac{ROI} filter usage (it is possible to filter the events by finding bounding boxes
that encapsulate light sources, but on a more complex scene this approach becomes relatively hard).
The other issue turned out to be the reflections of surrounding objects (as seen in \reffig{fig:meas1}), which caused
another source of unwanted events in the recording, which may in turn confuse some blob detection methods.

\begin{figure}[H]
	\centering
	\subfloat[An event-camera view of the UAV with UV LEDs.] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/meas1.png}
	  \label{fig:meas1_e}
	}
	\subfloat[View of the experiment setup.] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/meas1_c.png}
	  \label{fig:meas1_c}
	}
	\caption{
  The setup for measuring the event-camera response with a EVK4 camera. Visible reflections from a wall can be seen on \reffig{fig:meas1}. The setup is shown on \reffig{fig:meas1_c}.
  }
	\label{fig:meas1}
\end{figure}


\subsection{Distance - frequency influence}

In the following measurements, we use only one source of light, which is the whole end of the arm of the \ac{UAV} (with 2 \ac{UV} \ac{LED}s). Other arm light sources were turned off during the measurements.
Measurements were made in areas isolated by \ac{ROI} filter directly in Metavision Studio during recording, events were collected only in a
selected area around the light source, and the
rest of the events were discarded.
This time, the position of the \ac{UAV} was fixed relative to the camera on a blank background. The camera was placed on a tripod
and moved in increments of $0.2$ meters, starting from $1$ meter and ending at $3$ meters, with additional measurements made
at $4$ and $5$ meters.
The frequency range of the LED modulation was set in a range of $10$ Hz to $30$ kHz, with the blinking sequence set to \texttt{0, 1}.

\subsection{Rotation angle influence}

In addition to distance and frequency influence, the rotation angle influence also needs to be considered, to
verify the emitting characteristics of the light sources - if they can or cannot be considered lambertian.
The \ac{UAV} was rotated at increments of $45$ degrees relative to the event-based camera, at distances of $0.5$, $1$ and $2$ meters,
with frequencies ranging from $10$ Hz to $10$ kHz and the blinking sequence was set to \texttt{0, 1}.

%\subsection{RSSR Data collection}

%TODO: Rewrite this section

%Another dataset was collected for the application of \ac{RSSR} \cite{sooyongrssr}, which we analyze more in \refchap{chap:rssr}.
%The data includes calibration data, which is necessary for the optical system parameter estimation. This calibration is done by
%recording a video using a calibration lattice of LEDs with known spacing, and observing the pattern distortion in the
%resulting video.
%The UAV was placed at increasing distances and various angles relative to the event-based camera, with the LEDs blinking at frequencies different from
%each other. 
%The blinking sequences were set to the following values:
%\begin{lstlisting}
%	led_1 = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
%	led_2 = [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]
%	led_3 = [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
%	led_4 = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
%\end{lstlisting}
%with a common modulation frequency of $250$ Hz.
%This allows for the measurement of the ratio
%explain why to use the ratio, not the absolute value
%\footnote{Using the absolute value of the LED power is not suitable, as it also depends of the camera settings, surrounding
%environment and other factors. Finding such ratio (or property) that stays constant is crucial for correct distance estimation.}
%between the responses for each of the LEDs, which is necessary
%for the estimation of the UAV position using RSSR.

\section{Event response data processing}

The event-based camera response data was analyzed using the Metavision SDK\footnote{Metavision SDK Docs: \url{https://docs.prophesee.ai/stable/index.html}}
using its Python API. Each recording can be loaded as a raw file, producing a structured Numpy array of events, where each event is structured as
$(t, x, y, p)$. Specifically, $t$ represents the timestamp, $x$ and $y$ the spatial location of the event
on the camera sensor, and $p$ the polarity of the change in the detected brightness (compared to the previously recorded one).
%We can use many methods to analyze this data, such as

\subsection{Distance - frequency influence}

The distance frequency data set has recordings of the \ac{UAV} placed in front of the camera at distances $\mathcal{D}$ and with one \ac{LED} being modulated
at frequencies $\mathcal{F}$ \footnote{The frequencies represented in this list are the actual frequencies sent to the UVDAR unit. The preserved frequencies
are half of the values in this list - UVDAR interprets the frequency with a reference to the length of the sequence (here the sequence being \texttt{[0, 1]}).}. To minimize interference, only one LED was active during each recording, enabling isolated characterization of the diodeâ€™s response. The tested ranges were:
\[
\mathcal{F} = \{10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000, 20000, 30000\} \, \text{Hz},
\]
\[
\mathcal{D} = \{1.0 + 0.2k \mid k \in \{0, \dots, 10\}\} \cup \{4.0, 5.0\} \, \text{m}.
\]
%\begin{lstlisting}
%frequencies_Hz = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000, 20000, 30000]
%distances_m = [1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 4.0, 5.0]
%\end{lstlisting}
We can load the dataset into a matrix representing the distances and frequencies, then load a select number of events from each recording.
The data is then resampled into a 1D array by summing polarities over a selected bin width
\footnote{The bin width should be adjusted appropriately, as the farther the event-based camera is from the source, the fewer events are generated.}.
Peaks in this signal are then analyzed by SciPy's \texttt{findpeaks} function,
and the average number of events with the standard deviation is calculated for each frequency and distance.

We can see the influence of distance and frequency on the average number of events in \reffig{fig:dist} and \reffig{fig:freqs}, respectively. The data show a decreasing trend of the average number of events
with the increase of distance or frequency. The drop related to the distance can be explained by the perceived decrease in the intensity of the light source with increasing distance. With an increasing frequency, the camera cannot capture
all the changes that are generated by the light source, leading to a perceived drop in brightness.
On very high frequencies and distances, the camera is not able
to detect any real events at all, as there is more noise generated by the camera itself at this point. This can be observed
at \reffig{fig:dist} with a frequency of $30$ kHz at $3$ meters.

%\begin{figure}[H]
%	\centering
%	\subfloat[Influence of distance on the average number of events.] {
%	  \includegraphics[width=0.5\textwidth]{./fig/semestral/dist.pdf}
%	  \label{fig:dist_1}
%	}
%	\subfloat[Influence of distance on the log of average number of events.] {
%	  \includegraphics[width=0.5\textwidth]{./fig/semestral/distlog.pdf}
%	  \label{fig:dist_2}
%	}
%	\caption{
%  The influence of distance on the average number of events with the UAV rotated 0 degrees relative to the event-based camera on \reffig{fig:dist_1}, and with the log of the average number of events on \reffig{fig:dist_2}.
% }
%	\label{fig:dist}
%\end{figure}
%\begin{figure}[H]
%	\centering
%	\subfloat[Influence of frequency on the average number of events.] {
%	  \includegraphics[width=0.5\textwidth]{./fig/semestral/freq.pdf}
%	  \label{fig:freqs_1}
%	}
%	\subfloat[Influence of frequency on the log of average number of events.] {
%	  \includegraphics[width=0.5\textwidth]{./fig/semestral/freqlog.pdf}
%	  \label{fig:freqs_2}
%	}
%	\caption{
%  The influence of frequency on the average number of events with the UAV rotated 0 degrees relative to the event-based camera on \reffig{fig:freqs_1}, and with the log of the average number of events on \reffig{fig:freqs_2}.
%  }
%	\label{fig:freqs}
%\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{./fig/semestral/distlog.pdf}
    \caption{
        Influence of distance on the log of the average number of events, with the UAV rotated 0 degrees relative to the event-based camera.
    }
    \label{fig:dist}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{./fig/semestral/freqlog.pdf}
    \caption{
        Influence of frequency on the log of the average number of events, with the UAV rotated 0 degrees relative to the event-based camera.
    }
    \label{fig:freqs}
\end{figure}
If we now select one frequency and try to fit it with a curve,
we can observe that the data can be approximated with a rational or an exponential function, as shown in \reffig{fig:fit1}.
The best fit without being too complex is the inverse square law, which can be expressed as
\begin{equation}
	\text{intensity} \propto \frac{1}{\text{distance}^2}
\end{equation}
While more complex functions could be used to fit the data, they would likely lead to overfitting rather than capturing the underlying trend in a generalizable way, thus the inverse square law provides a good approximation of the data.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.80\textwidth]{./fig/semestral/inverse_square/square.pdf}
	\caption{Influence of distance data fitted with various curves.}
	\label{fig:fit1}
\end{figure}

\subsection{Rotation angle influence}
From the manufacturers datasheet of the used ProLight PM2B-1LLE 1W \ac{UV} Power \ac{LED} \footnote{The datasheet of ProLight PM2B-1LLE 1W UV Power LED can be obtained from \url{https://www.tme.eu/Document/9dfb498784ffdd07892a42f4f17c6f37/PM2B-1LLE-DTE.pdf}}
used in the UVDAR system, we can learn that the \ac{LED}s have a Lambertian radiation pattern,
which can be seen on \reffig{fig:lambertian}.
\begin {figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{./fig/semestral/lambertian/lambertian.pdf}
	\caption{Lambertian radiation pattern of the PM2B-1LLE UV LED.}
	\label{fig:lambertian}
\end{figure}
This means that the intensity of the light emitted from the LED decreases with the cosine
of the angle between the normal of the LED and the direction of the light \refeq{eq:lambertian}.
\begin{equation}
	I(\theta) = I_0\cos(\theta)
	\label{eq:lambertian}
\end{equation}
If we shift those distributions by $\pm 45$ degrees and sum them together, we can see the
theoretical distribution of the light emmited from the singular UAV arm \reffig{fig:lambert_combined}.
\begin {figure}[H]
	\centering
	\includegraphics[width=0.50\textwidth]{./fig/semestral/lambertian/3lambertian.pdf}
	\caption{Radiation pattern of two lambertian light sources shifted by $\pm 45$ degrees.}
	\label{fig:lambert_combined}
\end{figure}
With the dataset of the rotation of the UAV relative to the camera we will get the following results
on \reffig{fig:angles}.

\begin{figure}[H]
	\centering
	\subfloat[Influence of rotation of the UAV on the log of average number of events at 0.5 m.] {
	  \includegraphics[width=0.5\textwidth]{./fig/semestral/angle1.pdf}
	  \label{fig:angle_1}
	}
	% \subfloat[Influence of rotation of the UAV on the log of average number of events at 1 m.] {
	%   \includegraphics[width=0.5\textwidth]{./fig/semestral/angle2.pdf}
	%   \label{fig:angle_2}
	% }
	\subfloat[Influence of rotation of the UAV on the log of average number of events at 2 m.] {
	  \includegraphics[width=0.5\textwidth]{./fig/semestral/angle3.pdf}
	  \label{fig:angle_3}
	}
	\caption{
  The influence of rotation angle on the log of average number of events at 0.5 m on \reffig{fig:angle_1} and at 2 m on \reffig{fig:angle_3}.
  }
	\label{fig:angles}
\end{figure}
The data show a rough approximation of the theoretical distribution on \reffig{fig:lambert_combined},
but with a drop of intensity at the middle of the distribution. This could be caused
by the fact that LEDs, when close to the camera, can be perceived as multiple light sources,
but when moved further away, they merge into one source as shown on \reffig{fig:leds}.

\begin{figure}[H]
	\centering
	\subfloat[2 LEDs with blinking frequency of 10 Hz at 0.5 m.] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/2leds_05m.png}
	  \label{fig:leds_1}
	}
	\subfloat[2 LEDs with blinking frequency of 10 Hz at 2 m.] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/2leds_2m.png}
	  \label{fig:leds_2}
	}
	\caption{
  The light source on one arm of the UAV, consisting of two UV LEDs, blinking at a frequency of 10 Hz,
  placed at 0.5 m on \reffig{fig:leds_1} and 2 m at \reffig{fig:leds_2}.
  }
	\label{fig:leds}
\end{figure}
What we can also observe from \reffig{fig:leds} are the star-like shapes of the LEDs, which are supposed to be circular.
Those shapes are caused by light diffraction (and are named diffraction spikes), which are, in turn, caused by the aperture
blades in the lens of the camera. The number
of star spikes depend on the number of blades, the set aperture and the light source intensity then causes stars of different
levels of profoundness.\cite{lendermann2018computational} We can observe this by comparing how profound the star shapes are on different
frequencies, as shown on \reffig{fig:stars}.

\begin{figure}[H]
	\centering
	\subfloat[LED blinking at 10 Hz at 1.0 m] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/led_10hz.png}
	  \label{fig:stars_1}
	}
	\subfloat[LED blinking at 1 kHz at 1.0 m] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/led_1000hz.png}
	  \label{fig:stars_2}
	}
	\caption{
  Two same LED light sources at 1.0 meters, blinking at 10 Hz and 1 kHz.
  \reffig{fig:stars_1} shows a visible diffraction star (while being much brighter), while \reffig{fig:stars_2} shows a
  much more cicular source of light that is not as bright.
  }
	\label{fig:stars}
\end{figure}