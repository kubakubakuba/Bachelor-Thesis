%!TEX root = ../main.tex

\chapter{Introduction\label{chap:introduction}}

The rapid advancement of \ac{UAV} swarms has intensified the demand for robust and scalable relative pose estimation methods.
Traditional solutions relying on \ac{GNSS} suffer from limitations in indoor environments, signal occlusion, and interference that arises from
multi-agent communication.
\ac{VLP} systems, which leverage modulated \ac{LED} signals and optical sensors, offer a promising alternative due to their immunity to radio
frequency interference and high precision.

However, conventional frame-based cameras used in \ac{VLP} systems struggle with motion blur, latency,
and dynamic range constraints. For example, under bright illumination, \ac{LED}s may not produce a sufficiently detectable signal,
which may lead to localization failure. In contrast, event-based cameras overcome these limitations by asynchronously detecting pixel-level
brightness changes, providing microsecond temporal resolution, high dynamic range, and minimal latency. These attributes make them ideal for
capturing high-frequency LED signals, even in challenging lighting conditions or during aggressive \ac{UAV} maneuvers in agile swarming applications.

In this thesis, we present a method for estimating the pose of a \ac{UAV} equipped with \ac{UV} \ac{LED} light sources as in the UVDAR system. \cite{walteruvdar}
These \ac{LED}s are modulated at select frequencies, which aids in their identification and also helps with their prevalence
on the scene observed by the camera. After the camera is properly calibrated and the LED source locations are identified, a Perspective-n-Point 
algorithm is used to estimate the location of the \ac{UAV} in the 3D space. This estimation is then compared with the ground truth positions obtained
from the \ac{GNSS}.

\section{Related works}
Recent advances in relative pose estimation for UAV swarming applications
have focused on GNSS-denied environments and the overcoming of limitations
the navigation faces in these environments.
%In GNSS-denied environments various techniques, including visual odometry, radio communication

Shiba et al. \cite{Shiba25cvprw} released E-VLC dataset for visible light communication, a dataset combining an event camera, a frame camera, and synchronized ground-truth poses in various recording conditions for modulated visible-\ac{LED} communication and localization tasks.

Ebmer et al. \cite{ebmer2023} proposed an event-based camera pipeline for real-time 6-\ac{DOF} pose estimation using visible active LED markers. Their system achieved a latency below \SI{0.5}{\milli\second}, with a mean tracking accuracy of \SI{34.5}{\milli\meter} (translation) and \SI{0.738}{\degree} (rotation). The detection mode showed higher errors, with mean values of \SI{64.9}{\milli\meter} and \SI{1.55}{\degree} for translation and rotation, respectively. Standard deviations were \SI{16.2}{\milli\meter} and \SI{0.146}{\degree} for tracking, but increased significantly to \SI{121}{\milli\meter} and \SI{5.12}{\degree} for detection. Maximum observed errors reached \SI{87.8}{\milli\meter} (tracking) and \SI{1.233}{\meter} (detection) in translation, and up to \SI{71.9}{\degree} in rotation during detection.

Gou et al. \cite{GOU2025328} proposed a hybrid framework fusing depth-sensor data and event-based camera streams in a joint random-optimization scheme to achieve robust camera tracking and dense reconstruction under fast motion for agile robotics tasks.

\section{Contributions}
\todo{write this if necessary}

This section should describe the author's contributions to the field of research.

\section{Mathematical notation}

\todo{write this if necessary}

It is a good practice to define basic mathematical notation in the introduction.
See \reftab{tab:mathematical_notation} for an example.

\begin{table*}[!h]
  \scriptsize
  \centering
  \noindent\rule{\textwidth}{0.5pt}
  \begin{tabular}{lll}
    $\mathbf{x}$, $\bm{\alpha}$ & vector, pseudo-vector, or tuple\\
    $\mathbf{\hat{x}}$, $\bm{\hat{\omega}}$& unit vector or unit pseudo-vector\\
    %$\mathbf{\hat{e}}_1, \mathbf{\hat{e}}_2, \mathbf{\hat{e}}_3$ & elements of the \emph{standard basis} \\
    $\mathbf{X}, \bm{\Omega}$ & matrix \\
    $\mathbf{I}$ & identity matrix \\
    $\mathbf{R}$ & rotation matrix \\
    $\mathbf{t}$ & translation vector \\
    %$x = \mathbf{a}^\intercal\mathbf{b}$ & inner product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$\\
    %$\mathbf{x} = \mathbf{a}\times\mathbf{b}$ & cross product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$\\
    %$\mathbf{x} = \mathbf{a}\circ\mathbf{b}$ & element-wise product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$ \\
    %$\mathbf{x}_{(n)}$ = $\mathbf{x}^\intercal\mathbf{\hat{e}}_n$ & $\mathrm{n}^{\mathrm{th}}$ vector element (row), $\mathbf{x}, \mathbf{e} \in \mathbb{R}^3$\\
    %$\mathbf{X}_{(a,b)}$ & matrix element, (row, column)\\
    %$x_{d}$ & $x_d$ is \emph{desired}, a reference\\
    %$\dot{x}, \ddot{x}, \dot{\ddot{x}}$, $\ddot{\ddot{x}}$ & ${1^{\mathrm{st}}}$, ${2^{\mathrm{nd}}}$, ${3^{\mathrm{rd}}}$, and ${4^{\mathrm{th}}}$ time derivative of $x$\\
    %$x_{[n]}$ & $x$ at the sample $n$ \\
    %$\mathbf{A}, \mathbf{B}, \mathbf{x}$ & LTI system matrix, input matrix and input vector\\
    \emph{SO(3)} & 3D special orthogonal group of rotations\\
    %\emph{SE(3)} & \emph{SO(3)}~$\times~\mathbb{R}^3$, special Euclidean group\\
    $\delta(t)$ & Dirac delta function \\
  \end{tabular}
  \noindent\rule{\textwidth}{0.5pt}
  \caption{Mathematical notation, nomenclature and notable symbols.}
  \label{tab:mathematical_notation}
\end{table*}

%{\tiny\section{Document Statistics}\wordcount}