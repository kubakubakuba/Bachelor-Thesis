%!TEX root = ../main.tex

\chapter{Introduction\label{chap:introduction}}

The rapid advancement of \ac{UAV} swarms has intensified the demand for robust and scalable relative pose estimation methods.
Traditional solutions relying on \ac{GNSS} suffer from limitations in indoor environments, signal occlusion, and interference that arises from
multi-agent communication.
\ac{VLP} systems, which leverage modulated \ac{LED} signals and optical sensors, offer a promising alternative due to their immunity to radio
frequency interference and high precision.

However, conventional frame-based cameras used in \ac{VLP} systems struggle with motion blur, latency,
and dynamic range constraints. For example, under bright illumination, \ac{LED}s may not produce a sufficiently detectable signal,
which may lead to localization failure. In contrast, event-based cameras overcome these limitations by asynchronously detecting pixel-level
brightness changes, providing microsecond temporal resolution, high dynamic range, and minimal latency~\cite{gallego22event}. These attributes make them ideal for
capturing high-frequency LED signals, even in challenging lighting conditions or during aggressive \ac{UAV} maneuvers in agile swarming applications.

In this thesis a method for estimating the pose of an \ac{UAV} equipped with \ac{UV} \ac{LED} light sources - as used
in the UVDAR system~\cite{walteruvdar} is presented. These \ac{LED}s are observed by an event-based camera, with a fisheye lens. 
First, the \ac{LED} modulation frequency, distance, and rotation influence on the event-based camera response is analyzed;
this analysis informs the subsequent approach used.
After the camera is properly calibrated and the LED source locations are identified, a Perspective-n-Point 
algorithm is used to estimate the location of the \ac{UAV} in the 3D space. This estimation is then compared with the ground truth positions obtained
from the \ac{GNSS}.

%\todo{maybe write more}

\section{Related work}
%Recent advances in relative pose estimation for UAV swarming applications
%have focused on GNSS-denied environments and the overcoming of limitations
%the navigation faces in these environments.
%In GNSS-denied environments various techniques, including visual odometry, radio communication

Shiba et al.~\cite{Shiba25cvprw} released E-VLC dataset for visible light communication, a dataset combining an event camera, a frame camera, and synchronized ground-truth poses in various recording conditions for modulated visible-\ac{LED} communication and localization tasks.

Ebmer et al.~\cite{ebmer2023} proposed an event-based camera pipeline for real-time 6-\ac{DOF} pose estimation using visible active LED markers. Their system achieved a latency below \SI{0.5}{\milli\second}, with a mean tracking accuracy of \SI{34.5}{\milli\meter} (translation) and \SI{0.738}{\degree} (rotation). The detection mode showed higher errors, with mean values of \SI{64.9}{\milli\meter} and \SI{1.55}{\degree} for translation and rotation, respectively. The standard deviations were \SI{16.2}{\milli\meter} and \SI{0.146}{\degree} for tracking, but increased significantly to \SI{121}{\milli\meter} and \SI{5.12}{\degree} for detection. The maximum observed errors reached \SI{87.8}{\milli\meter} (tracking) and \SI{1.233}{\meter} (detection) in translation, and up to \SI{71.9}{\degree} in rotation during detection.

Gou et al.~\cite{GOU2025328} proposed a hybrid framework combining depth sensor data and event-based camera streams in a joint random optimization scheme. They achieved robust camera tracking and dense reconstruction under fast motion for agile robotics tasks.
They introduce an innovative 3D-2D edge alignment method tailored for event-based camera usage. Their approach achieves
robust performance even with high-speed camera motion exceeding 1 m/s.

Liu et al.~\cite{liu2024linebased6dofobjectpose} proposed a line-based object pose estimation method utilizing event-based cameras, 
by directly detecting object lines from event streams and performing pose optimization using robust estimation techniques.
This approach overcomes the challenge of noise interference inherent in event-based sensors by assigning different weights to events
based on their distance to the detected lines.
%\section{Contributions}
\section{Mathematical notation}

The following mathematical notation in \reftab{tab:mathematical_notation} is used, unless otherwise specified.

\begin{table*}[!h]
  \scriptsize
  \centering
  \noindent\rule{\textwidth}{0.5pt}
  \begin{tabular}{lll}
    %$\mathbf{x}$, $\bm{\alpha}$ & vector, pseudo-vector, or tuple\\
    $\mathbf{x}$, $\vec{x}$ & vector, pseudo-vector, or tuple\\
    %$\mathbf{\hat{x}}$, $\bm{\hat{\omega}}$& unit vector or unit pseudo-vector\\
    %$\mathbf{\hat{e}}_1, \mathbf{\hat{e}}_2, \mathbf{\hat{e}}_3$ & elements of the \emph{standard basis} \\
    $\mathbf{X}$ & matrix \\
    %$\mathbf{I}$ & identity matrix \\
    $\mathbf{R}$ & rotation matrix \\
    $\mathbf{t}$ & translation vector \\
    $x^{*}$ & optimal solution for $x$ \\
    %$x = \mathbf{a}^\intercal\mathbf{b}$ & inner product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$\\
    %$\mathbf{x} = \mathbf{a}\times\mathbf{b}$ & cross product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$\\
    %$\mathbf{x} = \mathbf{a}\circ\mathbf{b}$ & element-wise product of $\mathbf{a}$, $\mathbf{b}$ $\in \mathbb{R}^3$ \\
    %$\mathbf{x}_{(n)}$ = $\mathbf{x}^\intercal\mathbf{\hat{e}}_n$ & $\mathrm{n}^{\mathrm{th}}$ vector element (row), $\mathbf{x}, \mathbf{e} \in \mathbb{R}^3$\\
    %$\mathbf{X}_{(a,b)}$ & matrix element, (row, column)\\
    %$x_{d}$ & $x_d$ is \emph{desired}, a reference\\
    %$\dot{x}, \ddot{x}, \dot{\ddot{x}}$, $\ddot{\ddot{x}}$ & ${1^{\mathrm{st}}}$, ${2^{\mathrm{nd}}}$, ${3^{\mathrm{rd}}}$, and ${4^{\mathrm{th}}}$ time derivative of $x$\\
    %$x_{[n]}$ & $x$ at the sample $n$ \\
    %$\mathbf{A}, \mathbf{B}, \mathbf{x}$ & LTI system matrix, input matrix and input vector\\
    \emph{SO(3)} & 3D special orthogonal group of rotations\\
    %\emph{SE(3)} & \emph{SO(3)}~$\times~\mathbb{R}^3$, special Euclidean group\\
    $\delta(t)$ & Dirac delta function \\
    $\operatorname{Conv}(\cdot)$ & convex hull of points \\
  \end{tabular}
  \noindent\rule{\textwidth}{0.5pt}
  \caption{Mathematical notation, nomenclature and notable symbols.}
  \label{tab:mathematical_notation}
\end{table*}

%{\tiny\todo{REMOVE THIS\section{Document Statistics}\wordcount}}